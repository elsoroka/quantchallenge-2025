{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b572f3e",
   "metadata": {},
   "source": [
    "# Getting Started: Market Research\n",
    "This Jupyter notebook is a quick demonstration on how to get started on the market research section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb58a099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from torcheval.metrics import R2Score\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07d66aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '~/Desktop/localfiles/quantchallenge-2025/research/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9a964a",
   "metadata": {},
   "source": [
    "## 1) Download Data\n",
    "Please download the train and test data and place it within the ./research/data path. If you've placed it in the correct place, you should see the following cell work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdf4114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv(filepath+'data/train.csv')\n",
    "new_cols_train = pd.read_csv(filepath+'data/train_new.csv')\n",
    "train_data = pd.concat([train_data, new_cols_train], axis=1)\n",
    "\n",
    "test_data = pd.read_csv(filepath+'data/test.csv')\n",
    "new_cols_test = pd.read_csv(filepath+'data/test_new.csv')\n",
    "test_data = pd.concat([test_data, new_cols_test], axis=1)\n",
    "\n",
    "\n",
    "X_cols = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P']\n",
    "y_cols = ['Y1', 'Y2']\n",
    "\n",
    "print(train_data.head())\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88d58b3",
   "metadata": {},
   "source": [
    "## Emi's notes on the data\n",
    "* The biggest problem with this data is the distribution of Y1 and Y2 is not normal. The goal is to maximize $R^2$, and the corresponding loss function is $\\ell_2$ but this produces a normal distribution of errors between $\\hat y$ and $y$. Ideally, Y1 and Y2 should have normal distributions but they do not. Maybe resampling them will help?\n",
    "\n",
    "* You should probably standardize the column mean and std because on inspection, column A is much larger than the others. (See following cell.)\n",
    "\n",
    "* There are NAN values in O and P that have been replaced with 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846116d2",
   "metadata": {},
   "source": [
    "## Precondition the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b619a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "train_data.fillna(0, inplace=True)\n",
    "test_data.fillna(0, inplace=True)\n",
    "\n",
    "Xtrain = train_data[X_cols].to_numpy()\n",
    "ytrain = train_data[y_cols].to_numpy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(Xtrain)\n",
    "print(scaler.mean_)\n",
    "\n",
    "\n",
    "Xtest = test_data[X_cols].to_numpy()\n",
    "\n",
    "Xtrain = scaler.transform(Xtrain)\n",
    "Xtest = scaler.transform(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dff75fd",
   "metadata": {},
   "source": [
    "### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1d12e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, X:np.array, y:np.array):\n",
    "        self.N = X.shape[0]\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.float32)\n",
    "        print(X.shape)\n",
    "        print(y.shape)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.Tensor(self.X[idx,:]), torch.Tensor(self.y[idx,:])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc9a46a",
   "metadata": {},
   "source": [
    "## 2) Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66138f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.layer3 = nn.Linear(hidden_size, hidden_size//2)\n",
    "\n",
    "        self.layer4 = nn.Linear(hidden_size//2, output_size)\n",
    "    \n",
    "        # Xavier init\n",
    "        init.xavier_uniform(self.layer1.weight)\n",
    "        init.xavier_uniform(self.layer2.weight)\n",
    "        init.xavier_uniform(self.layer3.weight)\n",
    "        init.xavier_uniform(self.layer4.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.relu(self.layer3(x))\n",
    "        x = self.layer4(x)\n",
    "        return x\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430010d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic training loop\n",
    "\n",
    "def validate(model, validation_dataset, loss):\n",
    "    validation_dataloader = DataLoader(validation_dataset, shuffle=False, batch_size=1024)\n",
    "    losses = []\n",
    "    yhats = []\n",
    "    ys = []\n",
    "    metric = R2Score()\n",
    "    for X,y in tqdm(validation_dataloader):\n",
    "        yhat = model(X)\n",
    "        yhats.append(yhat.detach().numpy())\n",
    "        ys.append(y.detach().numpy())\n",
    "        metric.update(yhat, y)\n",
    "        l = loss(yhat, y)\n",
    "        losses.append(l.item())\n",
    "        \n",
    "    return losses, np.concatenate(yhats), np.concatenate(ys), metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12e37fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "model = NN(len(X_cols), len(X_cols), 2)\n",
    "n_epochs = 5\n",
    "losses = []\n",
    "learning_rate = 0.005\n",
    "best_r2 = -1.0\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.85)\n",
    "loss = torch.nn.MSELoss()\n",
    "\n",
    "validation_split = 1.0\n",
    "train_size = int(np.floor(Xtrain.shape[0]*validation_split))\n",
    "val_size = Xtrain.shape[0] - train_size\n",
    "train_dataset = SimpleDataset(Xtrain[:train_size,:], ytrain[:train_size,:])\n",
    "val_dataset = SimpleDataset(Xtrain[train_size:,:], ytrain[train_size:,:])\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for X,y in tqdm(DataLoader(train_dataset, shuffle=False, batch_size=48)):\n",
    "        yhat = model(X)\n",
    "        l = loss(yhat, y)\n",
    "        losses.append(l.item())\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    scheduler.step()\n",
    "    print(scheduler.get_last_lr())\n",
    "    \n",
    "    if validation_split < 1.0:\n",
    "        losses, yhats, ys, metric = validate(model, val_dataset, loss)\n",
    "        print(f\"Epoch {epoch}, R2: {metric.compute().item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e548a1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_weights.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430ba64a",
   "metadata": {},
   "source": [
    "## 3) Submit Predictions\n",
    "In order to submit predictions, we need to make a CSV file with three columns: id, Y1, and Y2. In the below example, we let our predictions of Y1 and Y2 be the means of Y1 and Y2 in the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20ed794",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = test_data[['id']]\n",
    "yhat = model(torch.Tensor(Xtest)).detach().numpy()\n",
    "print(Xtest.shape)\n",
    "print(yhat.shape)\n",
    "preds['Y1'] = yhat[:,0]\n",
    "preds['Y2'] = yhat[:,1]\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d072d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save preds to csv\n",
    "preds.to_csv('preds.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc988b0",
   "metadata": {},
   "source": [
    "You should now be able to submit preds.csv to [https://quantchallenge.org/dashboard/data/upload-predictions](https://quantchallenge.org/dashboard/data/upload-predictions)! Note that you should receive a public $R^2$ score of $-0.042456$ with this set of predictions. You should try to get the highest possible $R^2$ score over the course of these next few days. Be careful of overfitting to the public score, which is only calculated on a subset of the test dataâ€”the final score that counts is the private $R^2$ score!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
